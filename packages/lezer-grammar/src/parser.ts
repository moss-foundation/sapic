// This file was generated by lezer-generator. You probably shouldn't edit it.
import { LRParser } from "@lezer/lr";
import { rawTokenizer } from "./tokens";
import { highlight } from "./highlight";
export const parser = LRParser.deserialize({
  version: 14,
  states:
    "!dQQOROOO]OQO'#C_ObOQO'#CcOOOP'#Ch'#ChOOOP'#Cf'#CfQQOROOOgOQO,58yOlOQO,58}OOOP-E6d-E6dOOOP1G.e1G.eOOOP1G.i1G.i",
  stateData: "q~OPROSPOWQO~OTUO~OXVO~OUXO~OTYO~O",
  goto: "l]PPP^PPP^PPbPhTROTQTORWTTSOT",
  nodeNames: "âš  Raw Input Variable VarStart Identifier VarEnd PathParam Slash PathMarker",
  maxTerm: 12,
  propSources: [highlight],
  skippedNodes: [0],
  repeatNodeCount: 1,
  tokenData:
    "!z~RY!P!Qq!Q![v![!]!_!c!}v#R#Sv#T#ov#o#p!d#q#r!o#t;'Sv<%lOv~vOW~~{UT~!Q![v!c!}v#R#Sv#T#ov#t;'Sv<%lOv~!dOX~~!gP#o#p!j~!oOS~~!rP#q#r!u~!zOU~",
  tokenizers: [rawTokenizer, 0],
  topRules: { "Input": [0, 2] },
  tokenPrec: 0,
});
